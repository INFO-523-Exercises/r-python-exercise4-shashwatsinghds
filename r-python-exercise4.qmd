---
title: "Regression in R"
author: "Shashwat Singh"
format: html
editor: visual
---

# Loading Required Packages

```{r}
# Install necessary packages
if (!require(pacman))
  install.packages("pacman")

pacman::p_load(tidymodels,
               tidyverse,
               ranger,
               randomForest,
               glmnet,
               gridExtra)
# Setting up a global theme
theme_set(theme_bw() + theme(legend.position = "top"))

```

## Regression

Regression is a modeling technique for predicting quantitative-valued target attributes. The goals for this tutorial are as follows:

1. To provide examples of using different regression methods for the stock prices dataset.

2. To demonstrate the problem of model overfitting due to correlated attributes in the data.

3. To illustrate how regularization can be used to avoid model overfitting.


# Synthetic Data Generation

```{r}
# This block sets parameters for the analysis.
seed <- 1                   # Set the random seed for reproducibility
numInstances <- 200         # Define the total number of instances in the dataset

# Set seed
set.seed(seed)

# Generating data
X <- matrix(runif(numInstances), ncol=1)
y_true <- -3*X + 1 
y <- y_true + matrix(rnorm(numInstances), ncol=1)

```

- Here we have set seed and created Synthetic data randomly. We are using function which is relating y to x as y=3x+1. Also added gaussian noise.


```{r}
# Plotting the generated data to visualize the linear relationship

ggplot() +
  geom_point(aes(x=X, y=y), color="black") +
  geom_line(aes(x=X, y=y_true), color="blue", linewidth=1) +
  ggtitle('True function: y = -3X + 1') +
  xlab('X') +
  ylab('y')           # Create a scatter plot of the generated data


```


- The points represent the generated data, and the blue line shows the linear relationship between x and y without the additive gaussian noise.



## **Multiple Linear Regression**

Given a training set X,y MLR is designed to learn f(X,w)=X^T^w +w~0~ by minimizing the following loss function given a training set

L(y,f(X,w)) = (i=1 to n)âˆ‘ \|\|y~i-~ X~i~w-w~0~\|\|

where w (slope) and w0 (intercept) are the regression coefficients.

Given the input dataset, the following steps are performed:

1\. Split the input data into their respective training and test sets.

2\. Fit multiple linear regression to the training data.

3\. Apply the model to the test data.

4\. Evaluate the performance of the model.

5\. Postprocessing: Visualizing the fitted model.


## 1) Split Input Data into Training and Test Sets

```{r}
# This section splits the data into training and test sets
numTrain <- 20   # number of training instances
numTest <- numInstances - numTrain

set.seed(123) # For reproducibility



# Combine X and y into a tibble for easier data manipulation
data <- tibble(X = X, y = y)

split_obj <- initial_split(data, prop = numTrain/numInstances)



# Split the dataset into training and testing sets
train_data <- training(split_obj)
test_data <- testing(split_obj)

# Extract training and testing data from the split object

# Training sets
X_train <- train_data$X
y_train <- train_data$y

# Test sets
X_test <- test_data$X
y_test <- test_data$y
```

## 2) Fit Regression Model to Training Set

```{r}
# Setting up a linear regression model using the 'lm' engine
lin_reg_spec <- linear_reg() |> 
  set_engine("lm")

# Fit the linear regression model to the training data
lin_reg_fit <- lin_reg_spec |> 
  fit(y ~ X, data = train_data)

# Interpretation: 
# This step involves creating a linear regression model using the training data

```

## 3) Apply Model to the Test Set

```{r}
# This block applies the model to the test data

y_pred_test <- predict(lin_reg_fit, new_data = test_data) |>
  pull(.pred)

# Interpretation: 
# Involves predicting the response variable for the test set using the fitted model

```

## 4) Evaluate Model Performance on Test Set

```{r}
# This block evaluates the performance of the model on the test set

# Create a scatter plot to compare true and predicted values of the response variable for the test set
ggplot() + 
  geom_point(aes(x = as.vector(y_test), y = y_pred_test), color = 'black') +
  ggtitle('Comparing true and predicted values for test set') +
  xlab('True values for y') +
  ylab('Predicted values for y')


# Interpretation: 
# This part assesses how well the model predicts the response variable on new data

```

```{r}
# Organize test data and predictions for model evaluation
eval_data <- tibble(
  truth = as.vector(y_test),
  estimate = y_pred_test
)

# Calculate and display the Root Mean Squared Error (RMSE) and R squared value of the model
rmse_value <- rmse(data = eval_data, truth = truth, estimate = estimate)
r2_value <- rsq(eval_data, truth = truth, estimate = estimate)
```

```{r}
# Output the RMSE and R-squared value to assess the model's goodness of fit

cat("Root mean squared error =", sprintf("%.4f", rmse_value$.estimate), "\n")
cat('R-squared =', sprintf("%.4f", r2_value$.estimate), "\n")
```

## 5) Postprocessing

```{r}
# This block involves postprocessing steps after model evaluation

# Extract and display the slope coefficient from the linear regression model
coef_values <- coef(lin_reg_fit$fit) 
slope <- coef_values["X"]
intercept <- coef_values["(Intercept)"]

```

```{r}
# Output the intercept of the linear regression model
cat("Slope =", slope, "\n")

cat("Intercept =", intercept, "\n")
```

```{r}
# Plot the test data and the predicted linear function, displaying the model's fitted line

ggplot() +
  geom_point(aes(x = as.vector(X_test), y = as.vector(y_test)), color = 'black') +
  geom_line(aes(x = as.vector(X_test), y = y_pred_test), color = 'blue', linewidth = 1) +
  ggtitle(sprintf('Predicted Function: y = %.2fX + %.2f', slope, intercept)) +
  xlab('X') +
  ylab('y')




```
- Created training and test sets. Trained the model on the training data and determined the parameter for the linear regression.


# Effect of Correlated Attributes

## Create correlated attributes

```{r}
# Generate additional predictor variables (X2, X3, X4, X5) that are correlated with each other
set.seed(1)
X2 <- 0.5 * X + rnorm(numInstances, mean=0, sd=0.04)
X3 <- 0.5 * X2 + rnorm(numInstances, mean=0, sd=0.01)
X4 <- 0.5 * X3 + rnorm(numInstances, mean=0, sd=0.01)
X5 <- 0.5 * X4 + rnorm(numInstances, mean=0, sd=0.01)

```

```{r}
# Create and display scatter plots to visualize correlations between each pair of generated variables

plot1 <- ggplot() +
  geom_point(aes(X, X2), color='black') +
  xlab('X') + ylab('X2') +
  ggtitle(sprintf("Correlation between X and X2 = %.4f", cor(X[-c((numInstances-numTest+1):numInstances)], X2[-c((numInstances-numTest+1):numInstances)])))

plot2 <- ggplot() +
  geom_point(aes(X2, X3), color='black') +
  xlab('X2') + ylab('X3') +
  ggtitle(sprintf("Correlation between X2 and X3 = %.4f", cor(X2[-c((numInstances-numTest+1):numInstances)], X3[-c((numInstances-numTest+1):numInstances)])))

plot3 <- ggplot() +
  geom_point(aes(X3, X4), color='black') +
  xlab('X3') + ylab('X4') +
  ggtitle(sprintf("Correlation between X3 and X4 = %.4f", cor(X3[-c((numInstances-numTest+1):numInstances)], X4[-c((numInstances-numTest+1):numInstances)])))

plot4 <- ggplot() +
  geom_point(aes(X4, X5), color='black') +
  xlab('X4') + ylab('X5') +
  ggtitle(sprintf("Correlation between X4 and X5 = %.4f", cor(X4[-c((numInstances-numTest+1):numInstances)], X5[-c((numInstances-numTest+1):numInstances)])))

# Combine plots 
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

- There is noticeable correlation between all variables here specially between X2 and X3.
```{r}
# Split the dataset with correlated variables into training and test sets

train_indices <- 1:(numInstances - numTest)
test_indices <- (numInstances - numTest + 1):numInstances

# Create combined training and testing sets
X_train2 <- cbind(X[train_indices], X2[train_indices])
X_test2 <- cbind(X[test_indices], X2[test_indices])

X_train3 <- cbind(X[train_indices], X2[train_indices], X3[train_indices])
X_test3 <- cbind(X[test_indices], X2[test_indices], X3[test_indices])

X_train4 <- cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices])
X_test4 <- cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices])

X_train5 <- cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices], X5[train_indices])
X_test5 <- cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices], X5[test_indices])
```

```{r}
# Convert the training data matrices into tibble format for each model with increasing number of predictors

train_data2 <- tibble(X1 = X_train2[,1], X2 = X_train2[,2], y = y_train)
train_data3 <- tibble(X1 = X_train3[,1], X2 = X_train3[,2], X3 = X_train3[,3], y = y_train)
train_data4 <- tibble(X1 = X_train4[,1], X2 = X_train4[,2], X3 = X_train4[,3], X4 = X_train4[,4], y = y_train)
train_data5 <- tibble(X1 = X_train5[,1], X2 = X_train5[,2], X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5], y = y_train)

```

```{r}
# Fit linear regression models for each set of predictors, and adding more correlated variables

regr2_spec <- linear_reg() %>% set_engine("lm")
regr2_fit <- regr2_spec %>% fit(y ~ X1 + X2, data = train_data2)

regr3_spec <- linear_reg() %>% set_engine("lm")
regr3_fit <- regr3_spec %>% fit(y ~ X1 + X2 + X3, data = train_data3)

regr4_spec <- linear_reg() %>% set_engine("lm")
regr4_fit <- regr4_spec %>% fit(y ~ X1 + X2 + X3 + X4, data = train_data4)

regr5_spec <- linear_reg() %>% set_engine("lm")
regr5_fit <- regr5_spec %>% fit(y ~ X1 + X2 + X3 + X4 + X5, data = train_data5)
```

```{r}
# Convert training and test data matrices into data frames and perform predictions using the trained models

new_train_data2 <- setNames(as.data.frame(X_train2), c("X1", "X2"))
new_test_data2 <- setNames(as.data.frame(X_test2), c("X1", "X2"))

new_train_data3 <- setNames(as.data.frame(X_train3), c("X1", "X2", "X3"))
new_test_data3 <- setNames(as.data.frame(X_test3), c("X1", "X2", "X3"))

new_train_data4 <- setNames(as.data.frame(X_train4), c("X1", "X2", "X3", "X4"))
new_test_data4 <- setNames(as.data.frame(X_test4), c("X1", "X2", "X3", "X4"))

new_train_data5 <- setNames(as.data.frame(X_train5), c("X1", "X2", "X3", "X4", "X5"))
new_test_data5 <- setNames(as.data.frame(X_test5), c("X1", "X2", "X3", "X4", "X5"))

# Predictions
y_pred_train2 <- predict(regr2_fit, new_data = new_train_data2)
y_pred_test2 <- predict(regr2_fit, new_data = new_test_data2)

y_pred_train3 <- predict(regr3_fit, new_data = new_train_data3)
y_pred_test3 <- predict(regr3_fit, new_data = new_test_data3)

y_pred_train4 <- predict(regr4_fit, new_data = new_train_data4)
y_pred_test4 <- predict(regr4_fit, new_data = new_test_data4)

y_pred_train5 <- predict(regr5_fit, new_data = new_train_data5)
y_pred_test5 <- predict(regr5_fit, new_data = new_test_data5)
```

```{r}
# Extract model coefficients, calculate RMSE for train and test sets, and compute the sum of absolute weights for each model

# Extract coefficients and intercepts
get_coef <- function(model) {
  coef <- coefficients(model$fit)
  coef
}

# Calculate RMSE
calculate_rmse <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted)^2))
  rmse
}

results <- tibble(
  Model = c(sprintf("%.2f X + %.2f", get_coef(regr2_fit)['X1'], get_coef(regr2_fit)['(Intercept)']),
            sprintf("%.2f X + %.2f X2 + %.2f", get_coef(regr3_fit)['X1'], get_coef(regr3_fit)['X2'], get_coef(regr3_fit)['(Intercept)']),
            sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f", get_coef(regr4_fit)['X1'], get_coef(regr4_fit)['X2'], get_coef(regr4_fit)['X3'], get_coef(regr4_fit)['(Intercept)']),
            sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f", get_coef(regr5_fit)['X1'], get_coef(regr5_fit)['X2'], get_coef(regr5_fit)['X3'], get_coef(regr5_fit)['X4'], get_coef(regr5_fit)['(Intercept)'])),
  
  Train_error = c(calculate_rmse(y_train, y_pred_train2$.pred),
                  calculate_rmse(y_train, y_pred_train3$.pred),
                  calculate_rmse(y_train, y_pred_train4$.pred),
                  calculate_rmse(y_train, y_pred_train5$.pred)),
  
  Test_error = c(calculate_rmse(y_test, y_pred_test2$.pred),
                 calculate_rmse(y_test, y_pred_test3$.pred),
                 calculate_rmse(y_test, y_pred_test4$.pred),
                 calculate_rmse(y_test, y_pred_test5$.pred)),
  
  Sum_of_Absolute_Weights = c(sum(abs(get_coef(regr2_fit))),
                              sum(abs(get_coef(regr3_fit))),
                              sum(abs(get_coef(regr4_fit))),
                              sum(abs(get_coef(regr5_fit))))
)

```

```{r}
# Visualize the relationship between model complexity (sum of absolute weights) and error rates for training and testing sets

ggplot(results, aes(x = Sum_of_Absolute_Weights)) +
  geom_line(aes(y = Train_error, color = "Train error"), linetype = "solid") +
  geom_line(aes(y = Test_error, color = "Test error"), linetype = "dashed") +
  labs(x = "Sum of Absolute Weights", y = "Error rate") +
  theme_minimal()
```

```{r}
# Output a summary table of model equations, error rates, and sum of absolute weights

results

 
```

-The code focused on the impact of adding correlated predictors to linear regression models, revealing the trade-off between model complexity and accuracy in predictions


# Ridge Regression

```{r}
# Convert the training and testing sets into data frames for ridge regression

train_data <- tibble(y = y_train, X_train5)
test_data <- tibble(y = y_test, X_test5)
```

```{r}
# Define a ridge regression model with a specified penalty

ridge_spec <- linear_reg(penalty = 0.4, mixture = 1) %>% 
  set_engine("glmnet")
```

```{r}
# Fit the ridge regression model to the training data

ridge_fit <- ridge_spec %>% 
  fit(y ~ ., data = train_data)
```

```{r}
# Predict response variables for both training and testing sets using the ridge model

# Making predictions
y_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = test_data)$.pred


# Making predictions
y_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = train_data)$.pred

```

```{r}
# Calculate RMSE for ridge model predictions and extract model coefficients

# Calculate RMSE
calculate_rmse <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted)^2))
  rmse
}

# Extract coefficients
ridge_coef <- coefficients(ridge_fit$fit)

model6 <- sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f", 
                 ridge_coef[2], ridge_coef[3], ridge_coef[4], 
                 ridge_coef[5], ridge_coef[6], ridge_coef[1])

values6 <- tibble(
  Model = model6,
  Train_error = calculate_rmse(y_train, y_pred_train_ridge),
  Test_error = calculate_rmse(y_test, y_pred_test_ridge),
  Sum_of_Absolute_Weights = sum(abs(ridge_coef))
)
```

```{r}
# Combine the ridge regression results into a final summary table
final_results <- bind_rows(results, values6)

final_results



```

- Ridge regression is a statistical method for linear regression that is used to solve problems of multicollinearity. Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated with each other. This can make it difficult to estimate the true coefficients of the predictor variables, and can lead to inaccurate predictions.



# Lasso Regression

```{r}
# Define the lasso regression model with a specified penalty
lasso_spec <- linear_reg(penalty = 0.02, mixture = 1) %>% 
  set_engine("glmnet")
```

```{r}
# Prepare the training data in the required format for lasso regression

train_data <- tibble(y = y_train, X1 = X_train5[,1], X2 = X_train5[,2], 
                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])

```

```{r}
# Fit the lasso regression model to the training data

lasso_fit <- lasso_spec %>%
  fit(y ~ ., data = train_data)
```

```{r}
# Extract the coefficients from the fitted lasso model

lasso_coefs <- lasso_fit$fit$beta[,1]
```

```{r}
# Predict response variables for training and testing sets using the lasso model

y_pred_train_lasso <- predict(lasso_fit, new_data = train_data)$.pred
y_pred_test_lasso <- predict(lasso_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred

```

```{r}
# Convert the lasso model's coefficients into a readable string

model7 <- sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f", 
                  lasso_coefs[2], lasso_coefs[3], lasso_coefs[4], 
                  lasso_coefs[5], lasso_coefs[6], lasso_fit$fit$a0[1])

values7 <- c(model7, 
             sqrt(mean((y_train - y_pred_train_lasso)^2)),
             sqrt(mean((y_test - y_pred_test_lasso)^2)),
             sum(abs(lasso_coefs[-1])) + abs(lasso_fit$fit$a0[1]))
```

```{r}
# Compile the lasso regression results into a summary table

lasso_results <- tibble(Model = "Lasso",
                        `Train error` = values7[2], 
                        `Test error` = values7[3], 
                        `Sum of Absolute Weights` = values7[4])

lasso_results


```

- Lasso regression, also known as L1 regularization, is a type of linear regression that uses shrinkage to improve prediction accuracy and interpretability. It achieves this by adding a penalty term to the traditional least squares model, which encourages sparse solutions where some coefficients are forced to be exactly zero. This means that Lasso regression can effectively select relevant features and remove irrelevant ones, leading to a more interpretable model.